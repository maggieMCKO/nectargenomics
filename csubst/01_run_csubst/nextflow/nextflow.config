#!/usr/bin/env nextflow

params {
  config_profile_description = 'Maggie gwdg config'
  config_profile_contact = 'mcko@orn.mpg.de'
  // config_profile_url = 'https://hpc.nih.gov/apps/nextflow.html'
  // max_memory = '224 GB'
  // max_cpus = 32
  // max_time = '72 h'

  // igenomes_base = '/fdb/igenomes/'
}

// conda.enabled = true // only when we want to create conda env

// use a local executor for short jobs and it has to give -c and --mem to make nextflow
// allocate the resource automatically. For this the
// settings below may have to be adapted to the allocation for
// the main nextflow job.
executor {
    $local {
        queueSize = 10
        // memory = "$SLURM_MEM_PER_NODE MB"
        // cpus = "$SLURM_CPUS_PER_TASK"

    }
    $slurm {
        queue = 'medium'
        queueSize = 50
        pollInterval = '2 min'
        queueStatInterval = '5 min'
        submitRateLimit = '6/1min'
        retry.maxAttempts = 1
    }
}

process {
    executor = 'slurm'
    queue = 'medium'
    memory = '4 GB'
    time = '60 min'
    cpus = 2
}

profiles {
    gwdglocal {
        process.executor = 'local'
        process.cache = 'lenient'

    }

    gwdg {
        process {
            executor = 'slurm'
            maxRetries = 2

            clusterOptions = ' -C "scratch" '
            clusterOptions = ' -o log_nf_%A.out '
            clusterOptions = ' -e log_nf_%A.err '

            scratch = '/scratch/users/${USER}/$SLURM_JOBID'
            // with the default stageIn and stageOut settings using scratch can
            // result in humungous work folders
            // see https://github.com/nextflow-io/nextflow/issues/961 and
            //     https://www.nextflow.io/docs/latest/process.html?highlight=stageinmode
            stageInMode = 'symlink'
            stageOutMode = 'rsync'

            // for running pipeline on group sharing data directory, this can avoid inconsistent files timestamps
            cache = 'lenient'

        // example for setting different parameters for jobs with a 'gpu' label
        // withLabel:gpu {
        //    queue = 'gpu'
        //    time = '36h'
        //    clusterOptions = " --gres=lscratch:400,gpu:v100x:1 "
        //    containerOptions = " --nv "
        // }

        // example for setting different parameters for a process name
        //  withName: 'FASTP|MULTIQC' {
        //  cpus = 6
        //  queue = 'quick'
        //  memory = '6 GB'
        //  time = '4h'
        // }

        // example for setting different parameters for jobs with a resource label
        //  withLabel:process_low {
        //  cpus = 2
        //  memory = '12 GB'
        //  time = '4h'
        // }
        // withLabel:process_medium {
        //  cpus = 6
        //  memory = '36 GB'
        //  time = '12h'
        // }
        // withLabel:process_high {
        //  cpus = 12
        //  memory = '72 GB'
        //  time = '16 h'
        // }
     }
        timeline.enabled = true
        report.enabled = true
        trace.enabled = true
    }
}
